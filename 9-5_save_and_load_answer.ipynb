{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5761707c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2023, Acadential, All rights reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4c4eb7-3cf8-4649-8932-f1f77d4d5747",
   "metadata": {},
   "source": [
    "# 9-5. Save and Load Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9d2215-0c2d-4049-8804-cc438971fe68",
   "metadata": {},
   "source": [
    "학습된 모델을 저장하고 저장된 모델을 불러오는 과정을 살펴보겠습니다.\n",
    "\n",
    "Terminology:\n",
    "- checkpoint: 일반적으로 모델의 parameter을 저장한 파일을 의미. \n",
    "\n",
    "~~~\n",
    "# To save model\n",
    "torch.save(model, 'model.pth')\n",
    "\n",
    "# To load model\n",
    "model = torch.load('model.pth')\n",
    "~~~\n",
    "\n",
    "모델을 저장하고 불러오는 방법은 2가지 방법이 있습니다:\n",
    "1. 모델을 checkpoint에 **\"통째로\"** 저장하고 불러오는 방법.\n",
    "2. **모델의 parameter와 기타 다른 정보들 (test accuracy, 등등)** 을 checkpoint에 함께 저장하고 불러오는 방법\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe8f2272-b690-4fe1-96fd-b0613eff2c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from src.model import NeuralNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06921b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "243abd2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (fc_layers): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=196, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=196, out_features=49, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=49, out_features=10, bias=True)\n",
       "    (5): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f7a3582-a9d6-4a51-8b43-fa3d8640a053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "# checkpoints 폴더 생성 후 모델 저장\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "checkpoint_path = \"checkpoints/sample_model.pth\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54379197-c40b-4d8e-9db3-e7fa2754bac0",
   "metadata": {},
   "source": [
    "# 1. 모델을 통째로 저장하는 방법\n",
    "\n",
    "이 방법으로 모델을 load하려고 했을시 Neural Network model에 대한 python 코드가 없어도 괜찮습니다.\n",
    "\n",
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d13e21e0-a080-42d7-a98f-5c1b0b508efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(model, checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdc915f-31ef-4f1c-af6c-da99f7e8ebda",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0eeb2bc5-b0d7-4ac4-9dcd-13c0d4f911fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = torch.load(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cd43810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (fc_layers): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=196, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=196, out_features=49, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=49, out_features=10, bias=True)\n",
       "    (5): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13032e54-94cf-473b-9447-52502e219f37",
   "metadata": {},
   "source": [
    "# 2. 모델의 parameter와 기타 정보들을 함께 저장하는 방법\n",
    "\n",
    "이 방법으로 모델을 load하려면 Neural Network model을 먼저 initiate해야합니다. (즉, Neural Network model에 대한 python 코드가 필요함)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77484907-bb1c-4bed-b812-87c7b6ba9b16",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d123af3-90d7-4fee-b260-5152eb879736",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"checkpoints/sample_checkpoint.pt\"\n",
    "# 딕셔너리 형태로 명시\n",
    "content = {\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"epochs\": 100,\n",
    "    \"test_accuracy\": 0.9,\n",
    "    \"lr\": 0.001,\n",
    "}\n",
    "\n",
    "torch.save(content, checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19d669f-cd68-4131-b668-34ad65d4510d",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ace4e47-2d07-4d95-8f30-7e3fbf837485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model_state_dict', 'epochs', 'test_accuracy', 'lr'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_checkpoint = torch.load(checkpoint_path)\n",
    "loaded_checkpoint.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89ab870f-c9e2-41ae-a22b-7f8aa7650757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs = 100\n",
      "test_accuracy = 0.9\n",
      "lr = 0.001\n"
     ]
    }
   ],
   "source": [
    "for key in loaded_checkpoint.keys():\n",
    "    if key != \"model_state_dict\":\n",
    "        print(f\"{key} =\", loaded_checkpoint[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c527b54-93e1-4383-8e1e-6e5663720cbb",
   "metadata": {},
   "source": [
    "### Initiate NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "987e47b3-d572-41b6-8b35-d735b046aab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aae55ed5-67e4-44ac-823c-3208b1141ec3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.load_state_dict(loaded_checkpoint[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338c0af3-2b58-4c36-af35-0984e3f8753b",
   "metadata": {},
   "source": [
    "# Using pre-trained models from torchvision\n",
    "\n",
    "PyTorch의 torchvision에서는 다양한 Machine Vision Neural Network 모델들과 model checkpoint들을 제공합니다. \\\n",
    "대표적으로 제공되는 모델들:\n",
    "\n",
    "1. ResNet\n",
    "2. DenseNet\n",
    "3. MobileNet V2, V3\n",
    "4. Swin Transformer\n",
    "5. etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79af3ee9-c3cd-4025-9159-24b78b210ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "345955ed-d60d-405f-b3fc-a65e0b24e873",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Study\\Python\\DL\\venvDL\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "e:\\Study\\Python\\DL\\venvDL\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = resnet50(pretrained=False) # use randomly initiated weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42b6b61f-bbde-45ba-a5e3-b497f9f97b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Study\\Python\\DL\\venvDL\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to C:\\Users\\82104/.cache\\torch\\hub\\checkpoints\\resnet50-0676ba61.pth\n",
      "100%|██████████| 97.8M/97.8M [00:01<00:00, 56.9MB/s]\n"
     ]
    }
   ],
   "source": [
    "model = resnet50(pretrained=True)  # use ImageNet pretrained weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ded3c42-de3b-4455-b565-0f426c723347",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(torch.rand(1, 3, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03997466-8ed4-4c54-a9f4-e485b7af334f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1000])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a072b6-52fe-4506-9490-67b2089fbee0",
   "metadata": {},
   "source": [
    "# Advanced loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a0e82b-03b8-4514-8d10-a4cc21b92bec",
   "metadata": {},
   "source": [
    "만약에 pretrained된 모델의 weight을 사용하고 싶은데 마지막 classification layer만 randomly initialize하고 싶을때는 어떻게 할까요?\n",
    "\n",
    "예를 들어 ImageNet의 output class 개수는 1000개이지만 CIFAR 10의 경우 output class 개수는 10개입니다.\n",
    "\n",
    "그럴 경우 Last layer을 제외한 나머지 layer들의 weight들만 pretrained model checkpoint의 weight으로 initialize해줍니다! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5897e6ff-ced7-446b-ba96-7e4f481f8e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.resnet import ResNet50_Weights\n",
    "from torch.hub import load_state_dict_from_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83216634-8b11-4ddd-b2b1-bce8fde32ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to C:\\Users\\82104/.cache\\torch\\hub\\checkpoints\\resnet50-11ad3fa6.pth\n",
      "100%|██████████| 97.8M/97.8M [00:01<00:00, 56.7MB/s]\n"
     ]
    }
   ],
   "source": [
    "checkpoint = load_state_dict_from_url(ResNet50_Weights.IMAGENET1K_V2.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b849954c-da9e-4375-a099-85e8c2a5fcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet50(pretrained=False, num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b26c54b-c57b-4bec-bf41-989a5955812a",
   "metadata": {},
   "source": [
    "## Size Mismatch Error\n",
    "\n",
    "다음과 같이 output class 개수가 다르면 마지막 classification layer의 weight (matrix of shape (Hidden x Number of class) )가 다르기 때문에 에러가 뜹니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d5bb8b36-499b-454b-bc93-2edf8c3cfcb8",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ResNet:\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([1000, 2048]) from checkpoint, the shape in current model is torch.Size([10, 2048]).\n\tsize mismatch for fc.bias: copying a param with shape torch.Size([1000]) from checkpoint, the shape in current model is torch.Size([10]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Study\\Python\\DL\\venvDL\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ResNet:\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([1000, 2048]) from checkpoint, the shape in current model is torch.Size([10, 2048]).\n\tsize mismatch for fc.bias: copying a param with shape torch.Size([1000]) from checkpoint, the shape in current model is torch.Size([10])."
     ]
    }
   ],
   "source": [
    "model.load_state_dict(checkpoint, strict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0ed69d-b3eb-4c9e-947e-169ea4faedc3",
   "metadata": {},
   "source": [
    "## 마지막 Layer의 weight만 제외해서 checkpoint 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c76f8880-617b-4e71-9507-895c7f07cc57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layers to filter out ['fc.weight', 'fc.bias']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['fc.weight', 'fc.bias'], unexpected_keys=[])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "layers_to_filter_out = []\n",
    "model_state_dict = model.state_dict()\n",
    "\n",
    "# checkpoint의 레이어들을 순회하면서 각 레이어의 weight의 크기가 다르다면\n",
    "# FilterOut 대상이 되도록 해당 레이어는 LayersToFilterOut 리스트에 append\n",
    "# 해주도록 하겠습니다.\n",
    "for layer in checkpoint.keys():\n",
    "    if model_state_dict[layer].shape != checkpoint[layer].shape:\n",
    "        layers_to_filter_out.append(layer)\n",
    "# 어떤 레이어들이 filter out대상이 되었는지 확인하기 위해 프린트문을 작성해본다.\n",
    "\n",
    "print(\"Layers to filter out\", layers_to_filter_out)\n",
    "for layer in layers_to_filter_out:\n",
    "    del checkpoint[layer]\n",
    "\n",
    "model.load_state_dict(checkpoint, strict=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d215f3f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['conv1.weight', 'bn1.weight', 'bn1.bias', 'bn1.running_mean', 'bn1.running_var', 'bn1.num_batches_tracked', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.bn1.bias', 'layer1.0.bn1.running_mean', 'layer1.0.bn1.running_var', 'layer1.0.bn1.num_batches_tracked', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.bn2.bias', 'layer1.0.bn2.running_mean', 'layer1.0.bn2.running_var', 'layer1.0.bn2.num_batches_tracked', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.bn3.bias', 'layer1.0.bn3.running_mean', 'layer1.0.bn3.running_var', 'layer1.0.bn3.num_batches_tracked', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.0.downsample.1.bias', 'layer1.0.downsample.1.running_mean', 'layer1.0.downsample.1.running_var', 'layer1.0.downsample.1.num_batches_tracked', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.bn1.bias', 'layer1.1.bn1.running_mean', 'layer1.1.bn1.running_var', 'layer1.1.bn1.num_batches_tracked', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.bn2.bias', 'layer1.1.bn2.running_mean', 'layer1.1.bn2.running_var', 'layer1.1.bn2.num_batches_tracked', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.1.bn3.bias', 'layer1.1.bn3.running_mean', 'layer1.1.bn3.running_var', 'layer1.1.bn3.num_batches_tracked', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.bn1.bias', 'layer1.2.bn1.running_mean', 'layer1.2.bn1.running_var', 'layer1.2.bn1.num_batches_tracked', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.bn2.bias', 'layer1.2.bn2.running_mean', 'layer1.2.bn2.running_var', 'layer1.2.bn2.num_batches_tracked', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer1.2.bn3.bias', 'layer1.2.bn3.running_mean', 'layer1.2.bn3.running_var', 'layer1.2.bn3.num_batches_tracked', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.bn1.bias', 'layer2.0.bn1.running_mean', 'layer2.0.bn1.running_var', 'layer2.0.bn1.num_batches_tracked', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.bn2.bias', 'layer2.0.bn2.running_mean', 'layer2.0.bn2.running_var', 'layer2.0.bn2.num_batches_tracked', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.bn3.bias', 'layer2.0.bn3.running_mean', 'layer2.0.bn3.running_var', 'layer2.0.bn3.num_batches_tracked', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.0.downsample.1.bias', 'layer2.0.downsample.1.running_mean', 'layer2.0.downsample.1.running_var', 'layer2.0.downsample.1.num_batches_tracked', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.bn1.bias', 'layer2.1.bn1.running_mean', 'layer2.1.bn1.running_var', 'layer2.1.bn1.num_batches_tracked', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.bn2.bias', 'layer2.1.bn2.running_mean', 'layer2.1.bn2.running_var', 'layer2.1.bn2.num_batches_tracked', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.1.bn3.bias', 'layer2.1.bn3.running_mean', 'layer2.1.bn3.running_var', 'layer2.1.bn3.num_batches_tracked', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.bn1.bias', 'layer2.2.bn1.running_mean', 'layer2.2.bn1.running_var', 'layer2.2.bn1.num_batches_tracked', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.bn2.bias', 'layer2.2.bn2.running_mean', 'layer2.2.bn2.running_var', 'layer2.2.bn2.num_batches_tracked', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight', 'layer2.2.bn3.bias', 'layer2.2.bn3.running_mean', 'layer2.2.bn3.running_var', 'layer2.2.bn3.num_batches_tracked', 'layer2.3.conv1.weight', 'layer2.3.bn1.weight', 'layer2.3.bn1.bias', 'layer2.3.bn1.running_mean', 'layer2.3.bn1.running_var', 'layer2.3.bn1.num_batches_tracked', 'layer2.3.conv2.weight', 'layer2.3.bn2.weight', 'layer2.3.bn2.bias', 'layer2.3.bn2.running_mean', 'layer2.3.bn2.running_var', 'layer2.3.bn2.num_batches_tracked', 'layer2.3.conv3.weight', 'layer2.3.bn3.weight', 'layer2.3.bn3.bias', 'layer2.3.bn3.running_mean', 'layer2.3.bn3.running_var', 'layer2.3.bn3.num_batches_tracked', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.bn1.bias', 'layer3.0.bn1.running_mean', 'layer3.0.bn1.running_var', 'layer3.0.bn1.num_batches_tracked', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.bn2.bias', 'layer3.0.bn2.running_mean', 'layer3.0.bn2.running_var', 'layer3.0.bn2.num_batches_tracked', 'layer3.0.conv3.weight', 'layer3.0.bn3.weight', 'layer3.0.bn3.bias', 'layer3.0.bn3.running_mean', 'layer3.0.bn3.running_var', 'layer3.0.bn3.num_batches_tracked', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.0.downsample.1.bias', 'layer3.0.downsample.1.running_mean', 'layer3.0.downsample.1.running_var', 'layer3.0.downsample.1.num_batches_tracked', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.bn1.bias', 'layer3.1.bn1.running_mean', 'layer3.1.bn1.running_var', 'layer3.1.bn1.num_batches_tracked', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer3.1.bn2.bias', 'layer3.1.bn2.running_mean', 'layer3.1.bn2.running_var', 'layer3.1.bn2.num_batches_tracked', 'layer3.1.conv3.weight', 'layer3.1.bn3.weight', 'layer3.1.bn3.bias', 'layer3.1.bn3.running_mean', 'layer3.1.bn3.running_var', 'layer3.1.bn3.num_batches_tracked', 'layer3.2.conv1.weight', 'layer3.2.bn1.weight', 'layer3.2.bn1.bias', 'layer3.2.bn1.running_mean', 'layer3.2.bn1.running_var', 'layer3.2.bn1.num_batches_tracked', 'layer3.2.conv2.weight', 'layer3.2.bn2.weight', 'layer3.2.bn2.bias', 'layer3.2.bn2.running_mean', 'layer3.2.bn2.running_var', 'layer3.2.bn2.num_batches_tracked', 'layer3.2.conv3.weight', 'layer3.2.bn3.weight', 'layer3.2.bn3.bias', 'layer3.2.bn3.running_mean', 'layer3.2.bn3.running_var', 'layer3.2.bn3.num_batches_tracked', 'layer3.3.conv1.weight', 'layer3.3.bn1.weight', 'layer3.3.bn1.bias', 'layer3.3.bn1.running_mean', 'layer3.3.bn1.running_var', 'layer3.3.bn1.num_batches_tracked', 'layer3.3.conv2.weight', 'layer3.3.bn2.weight', 'layer3.3.bn2.bias', 'layer3.3.bn2.running_mean', 'layer3.3.bn2.running_var', 'layer3.3.bn2.num_batches_tracked', 'layer3.3.conv3.weight', 'layer3.3.bn3.weight', 'layer3.3.bn3.bias', 'layer3.3.bn3.running_mean', 'layer3.3.bn3.running_var', 'layer3.3.bn3.num_batches_tracked', 'layer3.4.conv1.weight', 'layer3.4.bn1.weight', 'layer3.4.bn1.bias', 'layer3.4.bn1.running_mean', 'layer3.4.bn1.running_var', 'layer3.4.bn1.num_batches_tracked', 'layer3.4.conv2.weight', 'layer3.4.bn2.weight', 'layer3.4.bn2.bias', 'layer3.4.bn2.running_mean', 'layer3.4.bn2.running_var', 'layer3.4.bn2.num_batches_tracked', 'layer3.4.conv3.weight', 'layer3.4.bn3.weight', 'layer3.4.bn3.bias', 'layer3.4.bn3.running_mean', 'layer3.4.bn3.running_var', 'layer3.4.bn3.num_batches_tracked', 'layer3.5.conv1.weight', 'layer3.5.bn1.weight', 'layer3.5.bn1.bias', 'layer3.5.bn1.running_mean', 'layer3.5.bn1.running_var', 'layer3.5.bn1.num_batches_tracked', 'layer3.5.conv2.weight', 'layer3.5.bn2.weight', 'layer3.5.bn2.bias', 'layer3.5.bn2.running_mean', 'layer3.5.bn2.running_var', 'layer3.5.bn2.num_batches_tracked', 'layer3.5.conv3.weight', 'layer3.5.bn3.weight', 'layer3.5.bn3.bias', 'layer3.5.bn3.running_mean', 'layer3.5.bn3.running_var', 'layer3.5.bn3.num_batches_tracked', 'layer4.0.conv1.weight', 'layer4.0.bn1.weight', 'layer4.0.bn1.bias', 'layer4.0.bn1.running_mean', 'layer4.0.bn1.running_var', 'layer4.0.bn1.num_batches_tracked', 'layer4.0.conv2.weight', 'layer4.0.bn2.weight', 'layer4.0.bn2.bias', 'layer4.0.bn2.running_mean', 'layer4.0.bn2.running_var', 'layer4.0.bn2.num_batches_tracked', 'layer4.0.conv3.weight', 'layer4.0.bn3.weight', 'layer4.0.bn3.bias', 'layer4.0.bn3.running_mean', 'layer4.0.bn3.running_var', 'layer4.0.bn3.num_batches_tracked', 'layer4.0.downsample.0.weight', 'layer4.0.downsample.1.weight', 'layer4.0.downsample.1.bias', 'layer4.0.downsample.1.running_mean', 'layer4.0.downsample.1.running_var', 'layer4.0.downsample.1.num_batches_tracked', 'layer4.1.conv1.weight', 'layer4.1.bn1.weight', 'layer4.1.bn1.bias', 'layer4.1.bn1.running_mean', 'layer4.1.bn1.running_var', 'layer4.1.bn1.num_batches_tracked', 'layer4.1.conv2.weight', 'layer4.1.bn2.weight', 'layer4.1.bn2.bias', 'layer4.1.bn2.running_mean', 'layer4.1.bn2.running_var', 'layer4.1.bn2.num_batches_tracked', 'layer4.1.conv3.weight', 'layer4.1.bn3.weight', 'layer4.1.bn3.bias', 'layer4.1.bn3.running_mean', 'layer4.1.bn3.running_var', 'layer4.1.bn3.num_batches_tracked', 'layer4.2.conv1.weight', 'layer4.2.bn1.weight', 'layer4.2.bn1.bias', 'layer4.2.bn1.running_mean', 'layer4.2.bn1.running_var', 'layer4.2.bn1.num_batches_tracked', 'layer4.2.conv2.weight', 'layer4.2.bn2.weight', 'layer4.2.bn2.bias', 'layer4.2.bn2.running_mean', 'layer4.2.bn2.running_var', 'layer4.2.bn2.num_batches_tracked', 'layer4.2.conv3.weight', 'layer4.2.bn3.weight', 'layer4.2.bn3.bias', 'layer4.2.bn3.running_mean', 'layer4.2.bn3.running_var', 'layer4.2.bn3.num_batches_tracked', 'fc.weight', 'fc.bias'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "48769d24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 7, 7])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint['conv1.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb0e1e9-0a45-4184-89bc-6ceb62c707eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfba5dc5-1c21-4dcc-972b-b4e34d7b3917",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
