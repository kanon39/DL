{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9153c9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2023, Acadential, All rights reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7da5a6",
   "metadata": {},
   "source": [
    "# 9-2. Fully Connected Neural Network PyTorch로 구현해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e318f1",
   "metadata": {},
   "source": [
    "## Import Torch library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1fbbf11-d50f-4cc5-9915-168b280a29ec",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b35351-fbc3-494b-b2ec-b71092adb0c0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Build a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80248d7-ff63-43c9-876f-1a02712a324f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### check device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17af05db-2964-4227-97f1-d29177d64d7f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e5789f-ae2c-4251-8b55-140a10e9527a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Define Model Class\n",
    "\n",
    "Section 2-7에서 살펴보았던 내용인데 다시 Recap해보면 다음과 같습니다.\n",
    "\n",
    "```nn.Module```을 사용하려면 기본적으로 다음 두 ```Method```을 Override 합니다:\n",
    "1. ```__init__``` : 신경망 모듈에서 사용되는 모든 모듈을 정의합니다.\n",
    "2. ```forward``` : 신경망 모듈에서 사용되는 모든 모듈의 연산을 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "184d9c1d-21e7-4ad4-9811-6747fa1c3654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뉴럴네트워크라는 NN.Module 기반의 클래스를 정리하였는데 여기 아래에\n",
    "# init과 forward 메서드가 각각 정의됩니다.\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        # Neural Network을 구성하는 layer들을\n",
    "        # initialize하는 부분\n",
    "        pass \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Neural Network의 forward pass을 정의하는 부분\n",
    "        # 이때 x은 input tensor\n",
    "        pass \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ddc726",
   "metadata": {},
   "source": [
    "## Neural Network을 구성할 각 Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a1349db",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "# 디버깅에활용될 샘플데이터 랜덤하게 생성\n",
    "# 미니배치 by 인풋피처 크기로 행렬을 만들게됨.\n",
    "# 8 images, 1 channel, 28x28 pixels\n",
    "sample_data = torch.rand(batch_size, 1, 28, 28)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95cd6057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened shape =  torch.Size([8, 784])\n"
     ]
    }
   ],
   "source": [
    "# Forward pass\n",
    "# flatten operation을 통해서 2차원 행렬로 변환해 주었다.\n",
    "# flatten을 첫번째 dimension부터 적용하라는 의미로 startDim은 1로 지정하였다.\n",
    "\n",
    "x = torch.flatten(sample_data, start_dim=1)\n",
    "print(\"Flattened shape = \", x.shape)\n",
    "\n",
    "# x shpae은 미니배치 by input feature의 갯수로 28x28이 flatten 되어서\n",
    "# 28제곱인 784가 된 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c2f698-dcd9-4d85-b829-cf5cab894501",
   "metadata": {},
   "source": [
    "### 참고 사항,\n",
    "뉴럴 레이어들을 차례대로 쌓게 되는데 출력되는 feature의 크기도 점진적으로 줄여 나가야 함.\n",
    "점진적으로 출력되는 feature 크기를 줄이는 이유?\n",
    "1. 맨 마지막에 최종적으로 출력되는 크기가 class의 개수가 되도록 하는 것\n",
    "2. 출력되는 feature 크기를 줄이는 것은 곧 Layer의 weight 개수를 줄이게 됨 → Model의 Complexity가 너무 커지는 것을 방지할 수 있고 이는 곧 Regularization의 효과\n",
    "3. Layer을 거듭할수록 풀려는 task와 관련된 추상적인 개념들을 학습하게 됨\n",
    "   - 예를 들어서 숫자 0은 꺽임이 없다거나 숫자 7은 뾰족하게 튀어나와 있는 특징들과 같은 추상적인 개념들\n",
    "   - 이러한 추상적인 개념들은 latent feature가 원래의 feature space보다 더 작은 차원의 subspace 상으로 mapping된다고 볼 수 있기 때문에 출력 feature 크기를 줄이는 것 (Manifold 이론)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e467699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 196])\n"
     ]
    }
   ],
   "source": [
    "# First FC Layer(첫번쨰  layer)\n",
    "fc1 = nn.Linear(784, 784 // 4) #(input,output)\n",
    "# 이 fc1을 텐서 X에 적용을 하게 되면 출력되는 X의 크기는 8x196이 되는 것을 확인할 수 있다.\n",
    "# 이 FC layer는 단순히 행렬의 곱이므로 여기는 non-linear가 없다.\n",
    "x = fc1(x)\n",
    "print(x.shape)  # torch.Size([8, 196])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d770930e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 196])\n"
     ]
    }
   ],
   "source": [
    "# ReLU Layer\n",
    "# 우리는 nonlinear로 ReLU 레이어를 지정해보겠다.\n",
    "# 이떄 ReLU는 X에 element wise(원소끼리 계산)되기 때문에 크기 변화없음.\n",
    "relu = nn.ReLU()\n",
    "x = relu(x) \n",
    "print(x.shape)  # torch.Size([8, 196])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8546612a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 49])\n"
     ]
    }
   ],
   "source": [
    "# Second FC + ReLU Layer\n",
    "# 두번째 레이어는 196 크기의 피처를 또 1/4크기인 49로 축소해주는 FC 레이어가 되겠다.\n",
    "fc2 = nn.Linear(784 // 4, 784 // 16)\n",
    "# Forward pass\n",
    "x = fc2(x)\n",
    "# apply relu\n",
    "x = relu(x)\n",
    "print(x.shape)  # torch.Size([8, 49])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3058aae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 10])\n"
     ]
    }
   ],
   "source": [
    "# Third FC + ReLU Layer\n",
    "fc3 = nn.Linear(784 // 16, 10)\n",
    "# Forward pass\n",
    "x = fc3(x)\n",
    "# apply sigmoid\n",
    "sigmoid = nn.Sigmoid()\n",
    "# 시그모이드 함수를 적용함으로써 각 로짓이 0에서 1사이의 값을 가지도록 구성하였다.\n",
    "x = sigmoid(x)\n",
    "print(x.shape)  # torch.Size([8, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795821a1",
   "metadata": {},
   "source": [
    "## Neural Network 정의\n",
    "앞서서 정의한 각 Layer들로 구성된 Neural Network를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bae2bd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        # Neural Network을 구성하는 layer들을\n",
    "        # initialize하는 부분\n",
    "        self.fc1 = nn.Linear(784, 784 // 4)\n",
    "        self.fc2 = nn.Linear(784 // 4, 784 // 16)\n",
    "        self.fc3 = nn.Linear(784 // 16, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Neural Network의 forward pass을 차례대로 정의하는 부분\n",
    "        # x은 input tensor\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0418a219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (fc1): Linear(in_features=784, out_features=196, bias=True)\n",
      "  (fc2): Linear(in_features=196, out_features=49, bias=True)\n",
      "  (fc3): Linear(in_features=49, out_features=10, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "996ce2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위 방법처럼도 구현할 수 있으나, 더 간단하게 nn.Sequential로 구현할 수 있다.\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        # Neural Network을 구성하는 layer들을\n",
    "        # initialize하는 부분\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(784, 784 // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(784 // 4, 784 // 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(784 // 16, 10),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Neural Network의 forward pass을 정의하는 부분\n",
    "        # x은 input tensor\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3c41e34-fe03-4925-bee2-e9e6674464d2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (fc_layers): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=196, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=196, out_features=49, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=49, out_features=10, bias=True)\n",
      "    (5): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bad688d-a6a9-40b9-b804-50f7bdb1468d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "23791d82-3a45-4075-9ff9-4270cc5b014d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([8, 10])\n",
      "Predicted class: tensor([2, 2, 2, 2, 2, 2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(8, 1, 28, 28, device=device)\n",
    "logits = model(X)\n",
    "print(f\"Logits shape: {logits.shape}\")\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "92fe7ea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 10])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cec191d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted probability 계산을 하려면SOFTMAX함수를 적용할 수 있다.\n",
    "pred_prob = nn.Softmax(dim=1)(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a31edd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# argmax는 그 확률 값을 클라스 인덱스 예측 값으로 변환해 주는 것이고\n",
    "# 여기서 softmax는 logit을 예측 확률 값으로 변환해 주는 연산으로 볼 수 있다.\n",
    "y_pred = pred_prob.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "33794b8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2333ca6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
